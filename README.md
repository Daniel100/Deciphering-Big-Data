# DECIPHERING BIG DATA

## CONTENT
1. Summary - Collaborative Discussion
2. Team Report
3. Executive Summary
4. Artefacts Unit 1: Big Data

## Summary - Collaborative Discussion 1: Internet of Things (IoT)

The exciting field of the Internet of Things (IoT) is defined by Gokhale et al. (2018) as the internet connected with physical objects, and it enables various items such as electronic devices and sensors (wearable health monitors, connected appliances, smart home security systems, logistics tracking) and more to collect and exchange data.

The collected and exchanged data from IoT devices can be very large and heterogeneous; it is a substantial contributor to Big Data, which is why there is a strong tie between IoT and Big Data (Cecchinel et al., 2014, as cited in Zainab et al., 2015: 41).

Marjani et al. (2017) highlight the benefits for organisations, businesses and individuals originating from the ability to analyse and utilise IoT data; for example, smart cities, smart transport and smart factory equipment, among others.

However, raw IoT data is mostly useless without the ability to analyse it (Marjani et al., 2017). Therefore, Sarkar (2019) stated the importance of data processing; this set of tasks is known as Data Wrangling.

Huxley (2020) elaborates on the quality of quantitative data and the ability to make the data usable for planned analysis.

Messy data can be viewed as unorganised or unstructured data; because most analyses require a tabular format (Huxley, 2020). I agree with Sieminski (2023, as cited in Yair 2022) that messy data can mean many things more than the one above, and Yair (2022) provides a more detailed definition. Sieminski (2023, as cited in Routhu 2022) states that sometimes messy data regarding structured data implies that the data needs to be transformed into an analysis-ready format; it may simply mean that the data must undergo a normalisation process.

Furthermore, data cleaning is crucial because otherwise, the risk is high that inaccurate results and conclusions are drawn from the analysis (Huxley, 2020).

Data Cleaning in the context of Big Data comes with limitations; it is time-consuming and requires domain expertise to achieve clean data (Ridzuan, 2019).

On the one side, humans outperform automated data-cleaning techniques (Geiger, 2023, as cited in Ding et al., 2022). On the other side, researchers, as mentioned in Geiger (2023, as cited in Shi et al. 2021), state that automated data cleaning tools improve data accuracy much faster than humans.

In Wong (2023, as cited in Horn, 2019) pointed out that humans are more likely to make errors than in automated data-cleaning processes.

Khalid (2023, as cited in Bhatt, 2020) pointed out that this time constraint mentioned above can be overcome by using data cleaning services, for example, by companies such as Experian (2021).

Tankard (2012) highlights the security issues in the context of Big Data since the data is primarily centrally stored in one place for analysis; data could be disclosed by human error or through attackers. Therefore, the data needs to be stored safely and protected, and all regulations need to be met (Tankard, 2012).

### REFERENCES:

Bhatt, V. (2020) The significance of data cleansing in Big Data explained, AiThority. Available from: [https://aithority.com/guest-authors/the-significance-of-data-cleansing-in-big-data/]() [Accessed: 11 February 2023].

Cecchinel, C. Jimenez, M. Mosser, S. & Riveill, M. (2014) An Architecture to Support the Collection of Big Data in the Internet of Things. IEEE World Congress on service: 442-449. DOI: 10.1109/SERVICES.2014.83

Ding, X. et al. (2022) IoT Data Cleaning Techniques. A Survey. Intelligent and Converged Networks 3(4): 325-339. DOI: 10.23919/ICN.2022.0026

Experian (2021) Case study: Royal Trinity Hospice - saves time and money with data cleansing, Experian UK. Available at: [https://www.experian.co.uk/blogs/latest-thinking/marketing/case-study-royal-trinity-hospice/]() [Accessed: 11 February 2023].

Geiger, J. (2022) Collaborative Learning Discussion 1: Peer Response Available from: [https://www.my-course.co.uk/mod/forum/discuss.php?d=135855]() [Accessed 11 February 2023].

Gokhale, P. Bhat, O. & Bhat, S. (2018) Introduction to IOT. International Advanced Research Journal in Science, Engineering and Technology 5(1): 41-44. DOI: 10.17148/IARJSET.2018.517

Huxley, K. (2020) ‘Data Cleaning’, in: Atkinson, P. (eds) SAGE Research Methods Foundations. London: SAGE Publications Ltd. Available from: [https://dx.doi.org/10.4135/9781526421036842861]() [Accessed 28 January 2023].

In Wong, H. (2023) Collaborative Learning Discussion 1: Peer Response Available from: [https://www.my-course.co.uk/mod/forum/discuss.php?d=135855]() [Accessed 11 February 2023].

Khalid, H. (2023) Collaborative Learning Discussion 1: Peer Response Available from: [https://www.my-course.co.uk/mod/forum/discuss.php?d=135855]() [Accessed 11 February 2023].

Marjani, M. et al. (2017) Big IoT Data Analytics: Architecture, Opportunities, and Open Research Challenges. IEEE Access 5: 5247-5261. DOI: 0.1109/ACCESS.2017.2689040

Ridzuan, F. Nazmee, W. M. & Zainon, W. (2019) A Review on Data Cleansing Methods for Big Data. Procedia Computer Science. 161: 731-738. DOI: 10.1016/j.procs.2019.11.177

Sarkar, D. T. Roychowdhury, S. (2001) Data Wrangling with Python. Packt Publishing. Available via Vitalsource Bookshelf. [Accessed 29 January 2023]

Sieminski, P. (2022) Collaborative Learning Discussion 1: Peer Response Available from: [https://www.my-course.co.uk/mod/forum/discuss.php?d=135855]() [Accessed 11 February 2023].

Tankard, C. (2012) Big data security. Network security. 7: 5–8. DOI: 10.1016/S1353-4858(12)70063-6

Yair, D. (2022) Your bible to structured vs. Unstructured Data, Bright Data. Available from: [https://brightdata.com/blog/how-tos/structured-vs-unstructured data?kw=&cpn=14745430544&cam=aw_all_products-all_geos-search_dsa_blog-kw_en-desktop_blog-how-tos__612826796311&utm_term=&utm_campaign=all_products-all_geos-search_dsa_blog-kw_en-desktop&utm_source=adwords&utm_medium=ppc&utm_content=blog-how-tos&hsa_acc=1393175403&hsa_cam=14745430544&hsa_grp=136943771953&hsa_ad=612826796311&hsa_src=g&hsa_tgt=dsa-1649388330704&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&cq_src=google_ads&cq_cmp=14745430544&cq_term=&cq_plac=&cq_net=g&cq_plt=gp&gclid=CjwKCAiAioifBhAXEiwApzCztq7RU_M_VKjByYivYWBtvbo0Ztza1ScLSkskwMSFwl2yj3BaAalsLBoCqr4QAvD_BwE#key]() [Accessed: 11 February 2023].

Zainab, H. A. Hesham, A. A. & Mahmoud M. B. (2015) Internet of Things (IoT): definitions, challenges and recent research directions. International Journal of Computer Applications 128(1): 37-47.


## Team Project

Project Team: Group 2

### INTRODUCTION
Our client, a successful barbershop located in the UK, requires measurement of the shop’s turnover to plan the budget, manage taxes and human resources. Furthermore, employee performance needs to be evaluated regularly since it is crucial for the clients success.

### CURRENT SITUATION
Currently, our client performs the time-consuming task of manually arranging, processing, and storing data in spreadsheets to create reports daily. This data comes from four separate sources: cash registers, customers, employees, and service. The system’s weakness lies in the disconnected data sources and the manual process.

### DATA PROCESS ARCHITECTURE
To tackle the lack of integration and manual process, the most beneficial solution is a fully automated and optimised cloud platform. This will enable the client to focus on other core areas whilst still generating reports and insights vital to the running of the business.

Our solution starts with setting up an e-commerce web application service for handling transactions, customer data, and services; it will be the single point of truth for raw data.

The next step is to set up a GCP account with BigQuery (free tier) because this will suit the small volume of data the client will be storing and processing. It is also equipped with top-tier security measures, managed directly by Google.

After the GCP account is created, data loads through an API from the e-commerce web application to the GCP.

Whilst the technical preconditions are now established, our data would still be considered raw data. Thus, a data pipeline must be implemented for transforming and cleaning the data.

The software service, DBT, will be used for the data engineering workflow - creating the ETL for these tasks. Furthermore, this tool has version control, documentation, and a testing framework which we will utilise to develop a reliable data pipeline resulting in a DWH.

Since data quality is crucial, we will implement a data testing strategy to detect inconsistencies such as outliers and formatting errors. Additionally, we will create user profiles for all used services to guarantee safe access management.

After this, the data is considered clean; automated queries will be created to gather the data. The data will be available via Google Sheets and guarantee our client a flexible way to analyse the data.

The graphical description of the data process can be found here .

The pipeline overview can be fount [here](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/team_project_hair_dresser_solution.png).

### PIPELINE DESCRIPTION
Can be found here, see [page 4](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/team_prject_hair_dresser_architecture.png).

### LOGICAL DATABASE
Can be found here, see [page 5-6](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/team_project_hairdressing_erd.png).

### REFERENCES
Handy, T. and Lantz, J. (2017) What, exactly, is DBT?, Transform data in your warehouse. Available at: https://www.getdbt.com/blog/what-exactly-is-dbt/ (Accessed: February 18, 2023).

Kimball, R. & Ross, M. (2013) The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling. 3rd ed. Indiana: John Wiley & Sons Inc.

HCL, T. L. (2022) MONEY(p,s) data type. Available from: https://www.ibm.com/docs/en/informix-servers/14.10?topic=types-moneyps-data-type [Accessed: February 19, 2023].

TechTarget, C. (2012) DEFINITION - fact table. Available from: https://www.techtarget.com/searchdatamanagement/definition/fact-table [Accessed: February 19, 2023].

## EXECUTIVE SUMMARY

Project Team: Group 2

### INTRODUCTION
This executive summary seeks to evaluate and synthesise the proposal for the completed design and build of the logical database tailored to the client’s needs. The proposal covered an explanation of the current client’s situation and proposed a solution to the client's issue. In this document, we will elaborate on what was discussed in the initial proposal in addition to focusing on the strengths and weaknesses of the proposed solution. Further, we will address the client’s legal and compliance requirements when it comes to the processing, handling, and storing of the data, paying particular importance to how to handle PII (Personally Identifiable Information), which is considered sensitive.

### CLIENT BACKGROUND
The client, a successful barbershop in the UK, needs an efficient way to measure its turnover to manage their budget, taxes, and human resources. Employee performance must also be evaluated regularly to ensure the business’s success. Currently, the client relies on a time-consuming manual process that involves arranging, processing, and storing data from four separate sources in spreadsheets to create daily reports. This process has the potential to be more efficient due to the disconnected data sources and the manual nature of the process.

The client has two main types of data flow:
One is mainly static and has slowly changing dimensions. This first type relates to information about employees and customers.
The second type is strictly transactional.
Looking further at the client’s data and after having assessed the past performance of the client, we learned that there are around 200 customers being served each month. There are eight hairdressers working full-time and no data specialists employed. Thus, this provides a rough idea of the volume of data; this information has been taken into consideration for the proposal.


### UNDERLYING CONCEPT
To address the client’s issues, implementing a data warehouse solution is apt. This is achieved in this case by using an e-commerce web application that sends the data through an API to Google Cloud Platform (GCP) and into the data warehouse (DWH). An open-source data transformation tool (DBT) is then used to automate the ETL (extract, transform, load) process, transforming the raw data into clean data. The graphical representation, see [here](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/team_project_hair_dresser_solution.png) .

A user interface will also be created to give the client a simple and easy way to access and analyse the data. This solution stores the data in one place and automatically transforms and structures the data. Furthermore, automated reports can be generated via Google Sheets from this data. Reports in Google Sheets are cloud-based and will automatically update for all of the accessing users. Whenever a report is accessed, all users will see the same, up-to-date information.

Utilising Google Sheets in place of a dedicated BI tool was a conscious decision aimed at decreasing the overall cost of implementation and maintenance; however, it has its drawbacks. The access rights and controls on Google Sheets can be easily changed even by a non-technical user. There is also a relatively high risk that the owner of the sheet will allow other users to make updates that may disrupt the structure of the report and, in turn, break the whole pipeline. This would then need the intervention of a data professional in order to be fixed. However, due to its simplicity and the automated process, Google Sheets is an adequate solution for the client’s reporting purposes.

Furthermore, using GCP provides one of the highest levels of security available among cloud providers. It also allows for implementing data compliance checks in the pipeline.

To summarise, the above has been chosen as a solution because the database design and technologies used for implementation are cheap or free in usage and maintenance. Relatively low data volumes processed by the barber shop make it possible to limit the costs related to the implementation of the database.

## CONCEPT’S STRENGTHS AND WEAKNESSES
The underlying concept of the proposed database model is a star schema; it consists of a fact table and is directly connected to dimensional tables (ThoughtSpot, 2022). Although the star schema has its drawbacks, we have opted to use it in our solution as its benefits outweigh its disadvantages for our application. For example, one such disadvantage is that the star schema has redundant data, which increases the storage on disk. This should not be an issue currently, but if the business scales, it may become a cost worth considering. This is caused by the denormalization of the data. Another disadvantage is that there is also a higher potential for errors in data as well as limited flexibility for non-dimensional data. Keboola (2022) states that, whilst the star schema provides benefits, it also has the limitations mentioned above and can be hard to maintain.

Additionally, although the pipeline setup is relatively easy for a data professional, it is not for a non-data-savvy person. Therefore, the maintenance of the pipeline (in case errors occur) may require the client to consult with or hire a professional each time the pipeline breaks, which generates additional costs for the business. This complexity (for non-technical users) also makes it hard and potentially costly to scale and extend existing reporting.

However, the advantages are that it simplifies data analysis, improves query performance, and enables reporting and scalability (Kimball & Ross, 2013: 16-18). The simplification of data analysis played a significant role in our decision to choose the star schema.

There are other advantages to the concept. For example, the data pipeline implements five layers in the processing architecture. This choice allows for improved readability of the code and implementation. It also allows for better control over the quality of the data. Each of the layers is responsible for a different, single operation (e.g., cleaning, transforming, and reporting), which is in line with software engineering principles (Singh, 2022).

This pipeline setup allows for full automation of the final reports, which are integrated with the data warehouse. This allows to save time on manually updating the reporting as well as allow for improved data governance. By the time the data reaches the final report, it has already gone through 4 layers on which quality testing and transformations are applied, ensuring that the final report contains valid and quality information.

## DEEP DIVE INTO THE TABLES’ STRUCTURE
The following section describes and explains the makeup of the tables and references to the appendix throughout:

The fact table is named fct_transactions (Appendix F) with all transactions; it stores all business process events and contains numerical data (Kimball & Ross, 2013: 41).

The dimensional tables hold information about explanatory attributes, and the dimensional tables’ primary keys point to the fact tables’ foreign keys (ThoughtSpot, 2022). The dimensional tables used in the database model are dim_service (Appendix A), dim_customer (Appendix B), dim_employee (Appendix D), and dim_payment (Appendix E).

All services and products provided by our client are listed in the dim_service table, and an e-commerce web application is used to manage the payments for the provided products and services. The payment information is stored in the table dim_payment (Appendix E).

Additionally, customer-related data are stored in the table dim_customer (Appendix B), and the country_code_glossary (Appendix C) contains all countries and their respective country code. These tables are linked together with the primary key country_code in the country_code_glossary (Appendix C) and the foreign key phone_country_code in the table country_code_glossary (Appendix C).

Our client needs the ability to evaluate its employees, whose data are stored in the dim_employee table (Appendix D).

The dimensional tables linked to the fact table, as discussed above, result in a star schema (Appendix G). In order to provide automated reports about employee performance and sales_report, the tables employee_monthly_performance_report (Appendix H) and sales_report (Appendix I) are needed. These two tables are on top of the star schema and reflect the monthly performance of their employees and sales in an aggregated form (Appendix J).


## LEGAL AND COMPLIANCE REQUIREMENT
To ensure legal compliance pertaining to data processing, the security of the data is paramount, as this has been proven to improve regulatory compliance (Kwon & Johnson, 2011). This does not mean that security equates to compliance; however, with strong security, compliance is easier to achieve (Comptia.org, 2016).

Some of the primary methods of security include encryption, backup and recovery, and access control (Rodrigues, 2023). Since the data is being stored and processed primarily using GCP, encryption is handled by Google (Default encryption at rest | documentation | googlecloud).Encryptionisvitalsincepersonal,sensitivedatafor employees and customers, such as email addresses and phone numbers, is being collected and processed in the database. Encryption ensures greater security for data. Backup and recovery are also available via a service in GCP. Additionally, access will be limited to the employees that require access in order to do their job (Byun & Li, 2006). This will also help ensure compliance, as only authorised users will be able to access the data.

By practising the above security methods, compliance with regulations such as GDPR becomes easier (McQuillian, 2022). For example, Art. 5 (f) of GDPR states that personal data shall be processed securely. Art. 6 details the lawfulness of processing the data; any customer or employee must provide consent for the processing of their data. Due to the nature of the business, agreements must be in place within the employee contract as well as either consent provided by any customer for storing their personal data or the data may fall into Art. 6 (b) i.e., if it is necessary for the performance of the contract (Regulation (EU) 2016/679 (General Data Protection Regulation), 2016).

Additionally, the rights of the data subjects must be considered, which are covered in Art. 12–23. In this case, the database must allow for the rectification and erasure of personal data for both employees and customers. Since this is stored in only a single table, it is relatively simple to remove and amend data upon request.


## CONCLUSION
The proposed solution leverages GCP in conjunction with processing data through an open-source software, dbt, to transform the data before providing reporting which is available in Google Sheets.

After weighing up the advantages and disadvantages of the star schema data model, it has been chosen as it ensures that the data is easy to understand and analyse; this model is suitable for the current size of the business as well as the volume of data.

Additionally, by using GCP, data security and compliance is made easier and the necessary steps have been taken to ensure that any PII is handled correctly. Only the management has access to the database and the reports currently, but if the business grows then there is a likelihood that the need would arise for more organised access controls. Further, the current design may reflect the current business requirements, but the business growing may necessitate more fields to be included and thus will be missing from the database and performance issues could arise if the database grows. Thus, the chosen solution may need to be adapted and upgraded in this event. This has been accounted for in relation to the scalability of both the GCP and the star schema.

## APPENDICES
The appendices can be here ; [page 6-10](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Executive%20Summary.pdf).


## REFERENCES
Byun, J.-W. and Li, N. (2006) “Purpose based access control for privacy protection in Relational Database Systems,” The VLDB Journal, 17(4), pp. 603–619. Available at: [https://doi.org/10.1007/s00778-006-0023-0]().

Default encryption at rest | documentation | google cloud (no date) Google. Google. Available at: [https://cloud.google.com/docs/security/encryption/default-encryption]() (Accessed: April 1, 2023).

Kimball, R. and Ross, M. (2013) The Data Warehouse toolkit the Definitive Guide to Dimensional Modeling. 3rd edn. Indianapolis, IN: Wiley.

Kwon, J. and Johnson, E. (2011) “The Impact of Security Practices on Regulatory Compliance and Security Performance,” International Conference on Information Systems [Preprint].

McQuillan, R. (2022) What is Data Access Control: In-depth guide, Budibase. Available at: [https://budibase.com/blog/app-building/data-access-control/]() (Accessed: April 1, 2023).

Quick start guide to security compliance: Cybersecurity: Comptia (no date) Compitia. Available at: [https://connect.comptia.org/content/guides/quick-start-guide-to-security-compliance]() (Accessed: April 1, 2023).

Rodrigues, J. (2023) Top 5 methods of protecting data, TitanFile. Available at: [https://www.titanfile.com/blog/5-methods-of-protecting-data]() (Accessed: April 1, 2023).

Singh, R. (2022) 8 software engineering principles to live by, CalliCoder. CalliCoder. Available at: [https://www.callicoder.com/software-development-principles/]() (Accessed: April 1, 2023).

Star schema vs snowflake schema and the 7 critical differences (2022) Star Schema vs Snowflake Schema and the 7 Critical Differences. Available at: [https://www.keboola.com/blog/star-schema-vs-snowflake-schema]() (Accessed: April 1, 2023).

Star schema vs snowflake schema: 6 key differences (2022) ThoughtSpot. ThoughtSpot. Available at: [https://www.thoughtspot.com/data-trends/data-modeling/star-schema-vs-snowflake-schema]() (Accessed: March 24, 2023).

Zola, A. (2021) What is a schema?, Data Management. TechTarget. Available at: [https://www.techtarget.com/searchdatamanagement/definition/schema]() (Accessed: March 24, 2023).

‘Regulation (EU) 2016/679 of the European Parliament and the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and the on the free movement of such data, and repealing Directive 95/46/EC’ (2016), Official Journal of the European Union L 119, pp. 1-88.


## API SECURITY REQUIREMENTS

Project Team: Group 2

### INTRODUCTION
Organisations and businesses increasingly use APIs to access data and services; the significance of API security cannot be overstated (axway, 2023). Insecure APIs are an attractive target for cybercriminals looking to steal data and carry out software attacks, as they are often the most exposed components of a network (Cobb, 2022). We will present below a security brief for an API based on the information we have about an API. The gaps in the information we have will be filled with the requirements that an API must meet in order to be secure.

### BUSINESS NEED
We are specifying the API security requirements for a common business case to gather Gross Domestic Product data. Gross Domestic Product (GDP) measures the value of goods and services produced in a country over a year. It's a key indicator of a country's economic performance and is used by companies to assess business conditions. Our team created a Python API to make it easier for organisations to access GDP data.

### API PROVIDER
The FEDERAL RESERVE BANK of ST. LOUIS is providing economic research data. We are using the API fredapi, a Python API that provides a function in Python that connects to the data services. An API key is needed to access the services provided by the API (FRED® API, N.D.).

### PYTHON CODE
This example code below shows the package import and simple data retrieval; full documentation can be found on GitHub (Mehyar, 2022).

```python 
api_key = "your key"

from fredapi import Fred
fred = Fred(api_key = api_key)

df = fred.get_series_all_releases('GDP')
df.head()

```



### SECURITY REQUIREMENTS SPECIFICATION


#### Authentication
The fredapi uses API keys to support secure authentication mechanisms. There is a requirement for setting up an account and providing a valid reason for the data retrieval.

#### Authorisation
FredAPI uses a role-based access control (RBAC) based on the account profile, allowing different users different access levels (Cobb, 2022). FredAPI’s team verifies credentials of eg. a University researcher and grants them a wider access than the one granted to an individual without valid reason.

#### Encryption
The API uses the secure communications protocol (HTTPS); this protocol includes standard protection (Cobb, 2022) for server requests (“GET”, “POST” etc.). The data does not have to be too carefully encrypted as it is public information and does not contain any PII.

#### Rate limiting
The API limits the number of requests per, for example, IP address in order to prevent abuse (Cobb, 2022).

#### Input validation
The API does allow for the data to be read only, there is no possibility to write into an API which prevents eg. SQL injections attacks.

#### Error handling
There should be unit testing in place that tests that correct errors are thrown and also that no sensitive information is being displayed as a part of the error thrown.

#### Logging and monitoring
The API logs all of the requests sent through the API in order to prevent malicious actors from abusing the API (Cobb, 2022). This helps to set up a monitoring and alerting system should any breach be detected and so the cyber security team can respond in a timely manner (axway, 2023).

#### Security Testing
Regular security testing, such as penetration testing and vulnerability assessment, must be conducted to identify and fix any vulnerabilities in the API. The API must implement appropriate data management controls, including access controls, backup and recovery, and retention policies, to ensure the security and integrity of data (Koshy, 2021).

#### Update process
There should be defined a process for updating the API in order to fix known vulnerabilities and ensure that the system remains secure as the technology develops.

#### Compliance
This API does not need to adhere to any major data privacy regulation (like GDPR, HIPAA or PCI-DSS) because it is not handling the type of data that would be covered by either of these regulations. However, any API should consider adherence to privacy regulations in order to make sure that the data is safely encrypted, masked where needed and that only authorised individuals can access any type of sensitive information.

### REFERENCES
axway (2023) API security: 12 essential best practices. Available from: [https://blog.axway.com/learning-center/digital-security/keys-oauth/api-security -best-practices]() [Accessed 8 April 2023].

Koshy, I. A (2021) API Security Testing: Importance, Risks and Best Practices. Available from: [https://beaglesecurity.com/blog/article/api-security-testing.html]() [Accessed 8 April 2023].

Cobb, M. (2022) 12 API security best practices to protect your business. Available from: [https://www.techtarget.com/searchapparchitecture/tip/10-API-security-gui delines-and-best-practices]() [Accessed 8 April 2023].

FRED® API (N.D.) FRED ECONOMIC DATA | ST. LOUIS FED. Available from: [https://fred.stlouisfed.org/docs/api/fred/]() [Accessed 8 April 2023].

Mehyar, M. (2022) fredapi: Python API for FRED (Federal Reserve Economic Data). Available from: [https://github.com/mortada/fredapi]() [Accessed 8 April 2023].

IBM QRADAR - Kaif Integration (no date) www.intellas.biz. Available at: [https://www.intellas.biz/case-studies/ibm-qradar-kaif-integration]() (Accessed: April 10, 2023).


## Unit 1: Big Data
This Unit covered big data and its associated technologies, concepts for data management and key characteristics. Big Data can be understood by its key characteristics: data grows exponentially, varied formats, boundlessness as well as high complexity and is often referred to as the V’s of Big Data (De Mauro et al., 2016). The V’s stand for: Volume (the data growth exponentially), Velocity (transmit data through the internet), Variety (different data types) and Veracity (data quality and availability) (Market Trends, 2021). This Unit helped me to understand the challenges that Big Data brings, especially in terms of data cleaning, data security and data privacy. Furthermore, I wrote an initial post about the data collection process Collaborative [Discussion 1](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Summary%20-%20Collaborative%20Discussion%201.pdf) .

### REFERENCES
De Mauro, A., Greco, M. & Grimaldi, M. (2016), "A formal definition of Big Data based on its essential features", Library Review 65(3): 122-135. DOI: https://doi.org/10.1108/LR-06-2015-0061

Market Trends (2021) The Four V’s of Big Data – What is big data? Analytics Insight. Available from: [https://www.analyticsinsight.net/the-four-vs-of-big-data-what-is-big-data/]() [Accessed 12 April 2023].

## Unit 2: Data Types
Data types and formats are the topics in Unit 2. Data types such as Characters, Strings, and Integers among others were introduced, as well as data formats such as structured, semi-structured, quasi-structured and unstructured (Sarkar & Roychowdhury, 2019). I learned a lot about basic data types and advanced data structures. I was also able to develop skills in Python, as a result of this module. See the following example:

```python 
# WORD COUNT

# the text
mText = ''' enter a text with serveral lines here '''

# check length and data type
print('data type: ', type(multiline_text))
print('length: ', len(multiline_text))

# delete all: punctations
mtext = mText.replace('\n', '')                            # removes line breaks
mtext = mtext.lower()
mtext = mtext.replace('.', '')                                      
mtext = mtext.replace(';', '')                                      
mtext = mtext.replace('?', '')                                      
mtext = mtext.replace('"', ' ')                                     
mtext = mtext.replace(',', '')                                      
mtext = mtext.replace("'", '')                                      
mtext = mtext.replace("!", '')
mtext = mtext.replace('  ', ' ')
mtext = mtext.split(' ')

# creatte a list with unique words
word_count = dict.fromkeys(mtext)

# count the words
for word in mtext:
    if word_count[word] is None:
        word_count[word] = 1
    else:
        word_count[word] += 1
        
print(word_count)

```



### REFERENCES
Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.


## Unit 3: Data Collection and Storage
This Unit encompassed data collection and storage. There are many scenarios where data needs to be collected from various sources such as governments, international organisations, education or research, NGOs, scientific data, or crowdsourcing (Kazil & Jarmul, 2016). Fact-checking the collected data is a necessary step in the process because it is important to know if the data is valid (Kazil & Jarmul, 2016). This task of fact-checking can be very time-consuming because it may be necessary to contact the source for verification, use other sources for comparison or investigate the topic further to make sure that the data is valid and well understood (Kazil & Jarmul, 2016). Subsequently, the data needs to be stored appropriately (Kazil & Jarmul, 2016).

I was able to learn a lot about external data sources (as mentioned above) and performed a web scraping task which included data cleaning and fact-checking (see [GitHub](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/web_scraping_gdp.ipynb). Furthermore, I wrote a comprehensive summary of the data collection process - see Collaborative Discussion 1 – The Data Collection Process

Furthermore, I wrote a comprehensive summary of the data collection process - see [Collaborative Discussion 1 – The Data Collection Process](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Summary%20-%20Collaborative%20Discussion%201.pdf).

### REFERENCES
Kazil, J., & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly Media, Inc. Available via the Vitalsource Bookshelf. [Accessed 05 March 2022].



## Unit 4: Data Cleaning and Transformation
The topic of this Unit was cleaning and transforming data. Huxley et al. (2020) describe data cleaning as the process of quantitatively checking the data to ensure accurate information. In addition, this unit also shows and analyses the steps of the data management pipeline: capturing raw data, data cleaning, data integration, database design, data analysis and data presentation/visualisation (EMC, 2015).

In this Unit I was able to deepen my knowledge of the various aspects of data cleaning and its importance in the data management pipeline. Furthermore, I completed the data management pipeline test.

### REFERENCES
EMC E. (2015) Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data. Wiley Professional Development Available via VitalSource Bookshelf [Accessed 12 April 2023].

Huxley, K. (2020) ‘Data Cleaning’, in: Atkinson, P. (eds) SAGE Research Methods Foundations. London: SAGE Publications Ltd. Available from: [https://dx.doi.org/10.4135/9781526421036842861]() [Accessed 28 January 2023].


## Unit 5: Data Cleaning and Automating
During this Unit I learned data cleaning and automatization with Python; how to load data from web pages, web scraping and how to perform single data cleaning on real live data. Furthermore, our team began working together on the team projects. Additionally, I performed several small data-cleaning tasks, see GitHub: [dataset 1](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Data-Cleaning%20house-prices.ipynb), [dataset 2](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Data-Cleaning-child-labour.ipynb).

## Unit 6: Data Design and Normalisation
This course was about database design as a process for creating a structured representation of data. This involves identifying data and defining relationships (Sarkar & Roychowdhury, 2019). Terms such as tables, attributes, keys, relations, and foreign keys are used in relational database construction (Mayo, 2023). The process of creating a logical database design involves normalisation to eliminate redundancy and inconsistencies and ensures data integrity (Munoz, 2022).

I learned a lot about database design and could apply this knowledge in the team project where I transformed a table (raw data) with student information into the 3rd - see GitHub for [raw table](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database_raw_table.png) , [1st](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database_first%20NF.png) , [2nd](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database_second%20NF.png) and [3rd NF](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database_third%20NF.png) .

### REFERENCES
Mayo, M. (2023): Database Key Terms, Explained. KDnuggets. Available from: [https://www.kdnuggets.com/2016/07/database-key-terms-explained.html](https://www.kdnuggets.com/2016/07/database-key-terms-explained.html) [Accessed 12 April 2023]

Munoz, A. (2022) Why is Database Normalization so Important? sales layer. Available from: [https://blog.saleslayer.com/why-is-database-normalization-so-important](https://blog.saleslayer.com/why-is-database-normalization-so-important) [Accessed 12 April 2023]

Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.


## Unit 7: Constructing Normalised Tables and Database Build
Units 6 and 7 are closely linked together regarding databases and how to normalise tables and transform them into the third normal form. I applied my knowledge to a real-life situation. My starting point was my local machine and a table with raw student data. I set up a database on my local machine (MariaDB) and created a Database and the schema Student; see SQL code on [GitHub](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database.sql) . I subsequently loaded the data into the student database and created an ERD from my database schema; see visualisation on [GitHub](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database.svg).



## Unit 8
This Unit covers the compliance and regulatory framework for managing data and emphasises the importance of standards, regulations, policy, controls, and the law. Standards, such as ISO27002, define a set of guidelines for information security that assist organisations in implementing, sustaining, and enhancing their information security management system (Cole, N.D.). Information security policies provide a framework for controlling the management of information and computer networks and can also be used for showing compliance with the law (isms.online). Abouelmehdi et al. (2017) recommend comprehensive data security in healthcare through the following steps: Authentication (controlled access), Encryption (prevent unauthorized access of sensitive data), Data Masking (replace sensitive data with unidentifiable values) and access control; this approach can be used for other areas as well in order to achieve a high level of data security.

I learned a lot from this Unit, and by studying the provided literature, I was able to fully comprehend the complexity of the subject.

### REFERENCES
Abouelmehdi, K. Beni-Hssane, A. Khaloufi, H. & Saadi, M. (2017) Big data security and privacy in healthcare: A Review. Procedia Computer Science. 113: 73-80. DOI: 10.1016/j.procs.2017.08.292

Cole, B. (N.D.) ISO 27002 (International Organization for Standardization 27002). Available from: [https://www.techtarget.com/searchsecurity/definition/ISO-27002-International-Organization-for-Standardization-27002](https://www.techtarget.com/searchsecurity/definition/ISO-27002-International-Organization-for-Standardization-27002) [Accessed 12 April 2023]

isms.online (N.D.) ISO 27002:2022, Control 5.1 – Policies for Information Security - ISO 27002:2022 Revised Controls. Available from: [https://www.isms.online/iso-27002/control-5-1-policies-for-information-security/](https://www.isms.online/iso-27002/control-5-1-policies-for-information-security/) [Accessed 12 April 2023]

Limmroth, S. (2020) Policies and Procedures: The Foundation for a Comprehensive HIPAA Program. Journal of Health Care Compliance 22(2), 41–46.


## Unit 9: Database Management System (DBMS)
Unit 9 covered Database Management System (DBMS). This is a crucial system software for creating and managing databases, and it enables end-users to create, protect, read, update, and delete data in a database (Mullins, ND). With the DBMS, data is consistently organised and easily accessible, and it serves as an interface between databases and users or application programs (Mullins, ND). Commonly used databases for relational data are MySQL, MariaDB, Microsoft SQL Server, Oracle DBMS, and PostgreSQL. Examples of semi-structured data (document-oriented) databases include MongoDB, Elasticsearch, and SQLite (Stackscale, 2022).

I am mastering this topic and understand the essential role of DBMS. Furthermore, I developed several databases during this course; see [student database](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Student_Database.sql) and [product database](https://github.com/Daniel100/Deciphering-Big-Data/blob/main/Product_Database.sql) on GitHub.

### REFERENCES
Mullins, C.S. (N.D.) database management system (DBMS). TechTarget. Available from: [https://www.techtarget.com/searchdatamanagement/definition/database-management-system](https://www.techtarget.com/searchdatamanagement/definition/database-management-system) [Accessed 12 April 2023].

Stackscale (2022) 10 popular database management systems (DBMS). Available from: [https://www.stackscale.com/blog/popular-database-management-systems/#MongoDB](https://www.stackscale.com/blog/popular-database-management-systems/#MongoDB) [Accessed 13 April 2023].



## Unit 10
text goes here.
## Unit 11
text goes here.
## Unit 12
text goes here.






